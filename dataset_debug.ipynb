{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug for Scientific data\n",
    "[PeerRead](https://github.com/allenai/PeerRead) (accept/reject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACL_path = './dataset/PeerRead/data/acl_2017'\n",
    "ICLR_path = './dataset/PeerRead/data/iclr_2017'\n",
    "coNLL_path = './dataset/PeerRead/data/conll_2016'\n",
    "arxiv_ai_path = './dataset/PeerRead/data/arxiv.cs.ai_2007-2017'\n",
    "arxiv_cl_path = './dataset/PeerRead/data/arxiv.cs.cl_2007-2017'\n",
    "arxiv_lg_path = './dataset/PeerRead/data/arxiv.cs.lg_2007-2017'\n",
    "subset = {'train' : 'train/parsed_pdfs/',\n",
    "         'test' : 'test/parsed_pdfs/',\n",
    "         'dev' : 'dev/parsed_pdfs/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed(path, subset):\n",
    "    parsed_data = {}\n",
    "    \n",
    "    for key,item in subset.items():\n",
    "        parsed_data[key] = glob.glob1(os.path.join(path, item),\n",
    "                                     '*')\n",
    "    meta = [(key, len(item)) for key,item in parsed_data.items()]\n",
    "    \n",
    "    return meta, parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, data = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['acl'], data['acl'] = get_parsed(ACL_path, subset)\n",
    "meta['iclr'], data['iclr'] = get_parsed(ICLR_path, subset)\n",
    "meta['coNLL'], data['coNLL'] = get_parsed(coNLL_path, subset)\n",
    "meta['ai'], data['ai'] = get_parsed(arxiv_ai_path, subset)\n",
    "meta['cl'], data['cl'] = get_parsed(arxiv_cl_path, subset)\n",
    "meta['lg'], data['lg'] = get_parsed(arxiv_lg_path, subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acl': [('train', 123), ('test', 7), ('dev', 7)],\n",
       " 'iclr': [('train', 349), ('test', 38), ('dev', 40)],\n",
       " 'coNLL': [('train', 19), ('test', 2), ('dev', 1)],\n",
       " 'ai': [('train', 3682), ('test', 205), ('dev', 205)],\n",
       " 'cl': [('train', 2374), ('test', 132), ('dev', 132)],\n",
       " 'lg': [('train', 4543), ('test', 253), ('dev', 252)]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis techniques requiring only word embeddings as features have proven to be successful (Socher et al., 2013; Tang et al., 2014; Iyyer et al., 2015). Research has also shown the importance of properly initializing the word embeddings, either through careful manipulation of the initialization of random vectors, or more commonly by pretraining the word embeddings (Kim, 2014). The content of the corpora used to train these word embeddings, however, has not been examined in depth.\n",
      "Most research tends toward using larger and larger datasets in order to build high-quality word embeddings, with some studies using corpora on the magnitude of several billion tokens (Mikolov et al., 2013; Pennington et al., 2014). This amount of data, however, is not available in all languages, nor is it necessarily an optimal use of available re-\n",
      "sources. In fact, there is evidence suggesting that tailoring a dataset for a task can achieve better results with less data (Suster et al., 2016; Barnes et al., 2016).\n",
      "The subjectivity or polarity content of a corpus could provide a good clue to the quality of word embeddings created for the task of sentiment analysis. Pang and Lee (2004) showed that using a pipeline technique where they first classified subjectivity before classifying polarity, they were able to achieve better results than performing classification without filtering subjective sentences. Therefore, we attempt to use subjectivity as a metric for the appropriateness of a corpus for our task.\n",
      "In this work we aim to:\n",
      "• discover if small task-specific datasets can provide better representations of words than large generic datasets for the task of sentiment analysis\n",
      "• quantify the amount of subjectivity in the data in order to predict the appropriateness of a dataset for the task of sentiment analysis.\n",
      "• discover the best way to combine this information for our task, given a large generic dataset and a smaller domain-specific dataset.\n",
      "• extract the subjective information as a way of approximating a smaller task-specific dataset, given only large generic datasets,\n",
      "As far as we know, this is the first work on the effects of the subjectivity and task-specificity of a corpus on the creation of word embeddings for the task of sentiment analysis. Given the importance of initialization and the wide-spread use of sentiment analysis techniques using word embeddings, the results are of importance to the community.\n",
      "2\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "This paper is organized as follows. Section 2 discusses previous work on subjectivity, generic word embeddings and task-specific word embeddings. Section 3 outlines how to quantify the subjectivity of a corpus. Section 4 determines how subjectivity and task-specificity effects word embeddings, and how to combine information from different datasets in order to improve classification. In Section 5 we propose a method to extract task-specific information from generic corpora and, by using the techniques from Section 4.2, improve over our baseline. Finally, in Section 6 we show that this technique of concatenation representations works even better for underresourced languages.\n",
      "\n",
      "Subjectivity can be a clear indicator of whether a sentence or phrase contains an opinion. In the case of movie or product reviews, this is especially relevant as there are often sentences that seemingly contain sentiment information, yet do not give an opinion towards any relevant aspect: for example, ”The main character tries to protect her good name.” Here, ’good’ gives no relevant sentiment information and could easily be found within a negative review.\n",
      "Pang and Lee (2004) use a subjectivity classifier to extract relevant sentences before passing them on to a sentiment classifier. Reducing the amount of irrelevant or potentially misleading data presumably leads to an improvement in polarity classification.\n",
      "This approach could potentially improve vector representations of words by providing only relevant examples to the word embedding algorithm.\n",
      "Representing words as feature vectors is an idea that has created interest for many years (Deerwester et al., 1990; Lund and Burgess, 1996). The simplest example is to represent a word as a onehot vector the length of the vocabulary, where all entries are zeros except for the index of the word, which is a 1. However, this simple technique does not allow for generalization, as the words ’dog’ and ’cat’ are as different as ’dog’ and ’dogs’.\n",
      "Lower-dimensional dense vectors, often known as word embeddings, are able to avoid this problem to a degree. They do this by attempting to\n",
      "force similar words to occupy a similar area in vector space, given an objective function. The most successful of these applications to date is the Skip Gram with Negative Sampling (SGNS) algorithm (Mikolov et al., 2013). This algorithm applies PMI factorization over an implicit word/feature matrix (Levy and Goldberg, 2014).\n",
      "Although generic word embeddings are often used as features for sentiment classifiers (Socher et al., 2013; Kim, 2014), there are good reasons to believe that the generality of these vectors may lead to inferior results. Most state-of-the-art sentiment classifiers are based on deep neural networks. With these architectures it is rather difficult, if not impossible, to find the global optimum and therefore, we must usually be content to find a local optimum. This fact also means that they are sensitive to the initialization of their word embeddings. For this reason, sentiment classifiers initialized with random vectors consistently underperform those initialized with pretrained vectors (Kim, 2014; Iyyer et al., 2015).\n",
      "Task-specific word embeddings are a further development of word embeddings. The purpose is to provide a representation that helps a classifier, often by removing or adding task-specific information.\n",
      "These vectors can be trained as normal and then retrofitted to the task (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015). The advantage of this technique is that any kind of word embedding can be used, as the vectors are modified post training. Faruqui et al. (2015) propose a method to incorporate information from semantic lexicons (WordNet, FrameNet, and ParaphraseNet) into word embeddings.\n",
      "Task-specific word embeddings can also be trained in a supervised manner, using the labels from annotated corpora (Maas et al., 2011; Tang et al., 2014, 2016) or task-specific dictionaries (Gouws and Søgaard, 2015). For the inclusion of sentiment information in word vectors, this is the most common approach.\n",
      "Maas et al. (2011) build a probabilistic model, similar to LDA (Blei et al., 2003), and attempt to model both word relationships and sentiment at the same time. However, their sentiment vectors performed worse than generic word vectors without the sentiment information.\n",
      "3\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "Tang et al. (2014) attempt to create sentimentspecific word vectors by modifying the feedforward network architecture used by Collobert and Weston (2011) with a hinge-objective that is designed to maximize both the word representation and the model’s ability to classify polarity. They collected 10 million distantly supervised tweets by searching for positive and negative emoticons. They trained a sentiment classifier and achieved state-of-the-art results on the SemEval2013 shared task (Nakov et al., 2013).\n",
      "Both of these techniques attempt to use large amounts of annotated data to extract sentiment information, but do not consider different sources of data or ways to combine them. However, it would be beneficial to examine more in depth the type of data that we use to create word vectors.\n",
      "The work that most closely resembles our is that of Joshi et al. (2015). They explore the effects of using in-domain and generic datasets to create embeddings for e-commerce NER. Their work shows that training on smaller amounts of task-specific data is better than training on large amounts of generic data. Unlike our work, they do not look into ways of combining information from different sources.\n",
      "\n",
      "Our first goal was to quantify the amount of subjective information contained in a corpus. We collected the following corpora, which we believe represent both generic datasets often used in the literature (Wikipedia, Europarl, Multi UN) as well as task-specific corpora (Amazon Movie Dataset). All corpora were sentence tokenized, word tokenized and lower-cased using Stanford Core NLP (Manning et al., 2014). We give the details of each corpus in the following subsections as well as in Table 1.\n",
      "Wikipedia Corpus\n",
      "After downloading a 2016 Wikipedia dump, we removed duplicates and markup using a freely available script1.\n",
      "1http://attardi.github.io/wikiextractor/\n",
      "English Europarl The English part of the Europarl v7 corpus2 (Koehn, 2005) is composed of around 2 million sentences from the European Parliament.\n",
      "Multi UN MultiUN3 is a corpus extracted from the official documents of the United Nations (Eisele and Chen, 2010). We use the English side of the English-Spanish corpus. It contains around 11 million sentences.\n",
      "Amazon Movie Corpus This dataset is composed of more than 7.5 million sentences taken from nearly 1.7 million reviews about movies (McAuley et al., 2015).\n",
      "In order to determine the amount of subjectivity contained in each corpus, we ran OpinionFinder on each of them. This is a system that was designed for subjectivity and polarity detection (Wilson et al., 2005). It has a high-precision and highrecall subjectivity classifier. The high-precision classifier uses a rule-based method, looking for well-established lexical clues. The high-recall classifier is a naive bayes classifier trained on the MPQA corpus. After running OpinionFinder on each corpus, we retrieved the results for highprecision subjectivity and high-recall subjectivity. In order to quantify the amount of subjectivity, we took the number of sentences reported as subjective and divide them by the total number of sentences in the corpus, (see Formula 1).\n",
      "Subjectivity Score = |subjective sentences| |total sentences|\n",
      "(1)\n",
      "This gives us an idea of the subjectivity content of one corpus compared to the others, although it is not a precise measurement. We did the same with high-precision and high-recall to get two scores for each corpus (see Table 1). As we expected, the Wikipedia corpus has the lowest scores for subjectivity. This is likely because of its encyclopedic format. The corpora with the highest subjectivity scores are the Amazon (high precision) and Europarl (high recall) corpora.\n",
      "2http://www.statmt.org/europarl 3http://opus.lingfil.uu.se/\n",
      "4\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "Wiki Multi UN Europarl Amazon High recall subj. 14.8% 33% 49% 41.5% High precision subj. 2.8% 5.1% 6% 9.3% # Sentences 118,859,889 11,350,967 1,965,734 7,540,838 # Tokens 2,099,860,107 322,056,452 54,474,544 162,390,631 Avg. Sent Length 17.7 28.4 27.7 21.5\n",
      "Table 1: Statistics of corpora used to train word embeddings.\n",
      "\n",
      "We created 50- 100- and 300-dimensional word vectors for each corpus using the Skip-gram with Negative Sampling algorithm (Mikolov et al., 2013) with a window of 10 words, and 5 negative samples.\n",
      "We tested the word vectors on the Rotten Tomatoes dataset (Pang and Lee, 2005), which contains 10662 annotated sentences. We train on 7996, leave 1066 as a development set and test on 1600.\n",
      "In order to see how well these representations work on a different domain, we also tested them on the English OpeNER sentiment corpus (Agerri et al., 2013). Specifically, we take the subcorpus from the hotel domain. It contains 3709 opinion phrases annotated for opinion holder, opinion target and four levels of sentiment (Strong Positive, Positive, Negative and Strong Negative). Each training example is a tuple of the opinion holder, opinion target and sentiment phrase, such as (”we”, ”hotel”, ”didn’t like at all”). We train on 2780, leave 186 as a development set and test on 743. This dataset has a class imbalance, which means that macro f1 is a more appropriate metric.\n",
      "Following the work of Iyyer et al. (2015), we use a Deep Averaging Network to perform sentiment classification. This neural bag-of-words model does not take the order of the words into consideration, but has proven to give results similar to more sophisticated approaches. Our datasets do not allow us to train on labeled phrases. Also the small size of the training data, especially in the OpeNER dataset, means that updating the word vectors while training the classifier actually hurts classification when using our task-specific vectors. Therefore, we do not update while training.\n",
      "For each sentence or phrase in the datasets, we create a training example. A training example is the average of the word embeddings for each word in the sentence or phrase. If a word is not found in\n",
      "our word embedding model, it is replaced with a generic ’unknown’ vector, which is randomly initialized. The results are shown in Figures 1 and 2.\n",
      "Figure 1: Accuracy of sentiment classifiers on Rotten Tomatoes dataset\n",
      "As we can see, the embeddings trained on the much smaller Amazon corpus give consistently better results than the Wikipedia corpus on the RT dataset. However, the vectors trained on Wikipedia perform quite well and even outperform their Amazon trained counterparts on the OpeNER dataset. This is likely due to the fact that the domain for the OpeNER dataset is different (Hotel reviews) compared to the RT dataset (Movie Reviews).\n",
      "Given that vectors trained on task-specific data regularly outperform those trained on much larger generic datasets, we should ask ourselves if it is possible to combine both of these datasets to get even better results. We compare three methods:\n",
      "1. Appending the task-specific dataset to the generic dataset and training one representation\n",
      "5\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "Figure 2: Accuracy of sentiment classifiers on OpeNER dataset\n",
      "2. Splicing the task-specific dataset into the generic dataset and training one representation\n",
      "3. Training two separate representations and combining them through vector concatenation.\n",
      "Appending involves simply adding the taskspecific Amazon Movie corpus to the Wikpedia corpus and training a single set of vectors on this data. This should improve the representations of those words that are important for the task, as they will receive more positive examples while training the vectors.\n",
      "Splicing is similar to appending, but instead of adding the Amazon sentences, we splice them into the Wikipedia corpus. We alternate sentences from the two corpora until we have exhausted all of the Amazon sentences, after which we add the remaining Wikipedia sentences. As above, we train a single set of vectors.\n",
      "For concatenation, we train one set of vectors on Wikipedia and a second on the Amazon Movie Corpus separately. Given two m dimensional vectors, we concatenate them, resulting in an m +m dimensional vector, in order to provide both views to the sentiment classifier. For a fair comparison, we concatenate vectors that are half as large as those of the other techniques. This technique may provide better representations for words that often appear in one corpus but not in the other.\n",
      "Note that the embeddings in these three techniques were trained on exactly the same data, the only difference being the method of combining\n",
      "this data. Our baseline vectors are trained only on the Wikipedia corpus. We also test the 50 dimensional sentiment embeddings (SSWEu) provided by Tang et. al (2014) (see Section 2.3). The results are shown in Table 2.\n",
      "We can see that including task-specific data with any of the techniques tends to improve over the baseline. This in itself is not surprising. However, it is important to notice that the concatenation of two vectors trained on the two datasets separately always improve the macro f1 score.\n",
      "Given that we can combine generic and taskspecific data to improve our feature space, we would like to know how much task-specific data is necessary to see these improvements. Therefore, we perform experiments on the RT dataset with increasing amounts of task-specific data. The results are shown in Figure 3.\n",
      "As we can see from the results, we see improvements over the baseline after 10-12 million tokens. This could potentially benefit low-resource languages which often have access only to Wikipedia pages and little task-specific data.\n",
      "Finally, we ask if there is a way to approximate this task-specific data given only large, generic corpora. This again is a situation that is very common among under-resourced languages. There-\n",
      "6\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "RT OpeNER 50 100 200 300 50 100 200 300\n",
      "Wikipedia Baseline acc 66.2 71.9 71.4 71.3 77.6 78.2 82.1 83.6 f1 59.8 72.4 .67.4 .67.5 37.2 36.7 39.8 40.6 SSWEu acc 66.3 – – – 81 – – – f1 56.5 – – – 39.4 – – – Amazon acc 73.8 76.3 76.7 76.9 73.5 73.8 77.8 76.9 f1 69.9 74.7 74.6 75.3 34.9 33.6 37.4 38 Appended acc 69.4 73.8 74.5 74.3 77.6 76.5 82.7 78.4 f1 65.1 75 72.5 72.8 36.9 35.8 40.5 36.4 Spliced acc 70.8 71.2 73.1 73.6 77.4 73.9 84.6 82.7 f1 62.5 73.4 71 70 37.2 32.9 41.4 39.9 Amazon + Wiki acc 74.4 75.2 75.9 75.9 64.2 65.7 69.4 72.3 f1 72.5 75 78 77.7 57.8 62.4 63.8 68\n",
      "Table 2: Accuracy and F1 of sentiment classifier with embeddings trained on Wikipedia and Amazon corpora with different techniques. For the OpeNER dataset, accuracy is the macro average. Bold indicates the highest performance and underline indicates those that beat the generic baseline.\n",
      "fore, we extract the sentences labeled as subjective from the Wikipedia, Europarl and Multi UN corpora, referred to from here on as subj-Wikipedia, subj-Europarl, subj-MultiUN. We train word embeddings on each of these and test them as in Section 4.2. We also report macro-averaged f1 scores. Results are shown in Table 3.\n",
      "We can see that by filtering the corpora for subjectivity, we lose a degree of accuracy, but gain in f1. The poor f1 scores of the Wikipedia baseline on the OpeNER dataset are due to the fact that the classifier nearly always chooses the two majority classes and completely ignores the others. Including more subjective information seems to ameliorate this situation. The improvements in f1 also coincide with the subjectivity level predicted by our method. This suggests that it is possible to predict the amount of improvement that you can gain by extracting subjective data from a generic dataset.\n",
      "Given that the classification error is still relatively large, our hypothesis was that many words which are important for classification are not found in the corpora used to train the word embeddings. In order to test this possibility, we found the lexical overlap in vocabulary between the corpora used to train the word embeddings and the RT and OpeNER corpora. Let Vtr be the set of words in a training corpus and Vtest be the set of words in each test corpus test.\n",
      "LexicalOverlap = |Vtest| ∩ |Vtr| |Vtest|\n",
      "(2)\n",
      "If the lexical overlap of the corpora used to train our representations (Wikipedia, Europarl, Multiun) and the datasets we use to test our classifier (RT, OpeNER) is small, this will negatively affect the results of the classifier, as it will lack necessary information during both training and testing. We also test for lexical diversity of the filtered subjectivity training corpora as a measure of vocabulary. We use the Average Type Token Ratio (ATTR) as a measure. Let k be a corpus, nk be a subset of n tokens of k, Vn = the vocabulary of n, and Tn = the tokens of n.\n",
      "Average Type Token Ratio = 1\n",
      "n ∗ N∑ n=1 |Vn| |Tn| (3)\n",
      "We set n to 1000. The results are shown in Table 4.\n",
      "As we can see in Table 4, the number of words that do not have a vector representation in the full Wikipedia model is much smaller than all other models. Therefore, the Wikipedia word embeddings have a larger vocabulary yet they do not always have a representation that is helpful for the task. The more subjective datasets may lack some vocabulary, but it seems that the representations that they have are able to overcome the sparsity.\n",
      "7\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "RT OpeNER 50 100 200 300 50 100 200 300\n",
      "Wikipedia Baseline acc 66.2 71.9 71.4 71.3 77.6 78.2 82.1 83.6 f1 59.8 72.4 67.4 67.5 37.2 36.7 39.8 40.6 subj-Wiki + Wiki acc 66.9 69.1 70.3 72.4 62.1 67.6 67.8 70.3 f1 67.5 71.8 74.6 75.5 51.2 58.3 57.9 60.8 subj-Multiun + Wiki acc 68.4 69.6 69.3 73.4 65.1 68.3 68.2 71 f1 69 73.2 74 75.6 60.2 60.7 57.7 62.8 subj-Europarl + Wiki acc 68.2 68.4 71.3 74.1 63.7 68.3 69.6 71.7 f1 69.7 73.7 75.5 76.2 56.9 59.5 66.3 63.4\n",
      "Table 3: Accuracy and F1 scores of sentiment classifier trained on concatenation of filtered subjectivity vectors and wikipedia vectors. Bold indicates the highest performance and underline indicates those that beat the Wikipedia baseline.\n",
      "Wiki Amazon subj-Wiki subj-MulUN subj-Euro # tokens 1.8B˜ 162M˜ 24M˜ 43M˜ 33M˜ ATTR 41.5 39.9 44.3 36.7 39.4 Overlap RT-Train 83.7 61.6 73.3 70.9 68.6 Overlap RT-Test 87.5 69.9 76.7 75.3 74 Overlap OpeNER-Train 89.1 70.3 81.3 78.1 75 Overlap OpeNER-Test 96 89.1 92 90 90\n",
      "Table 4: Average Type/Token ratio (ATTR) and overlap with test vocabulary in each training corpus.\n",
      "After seeing the number of words which were missing, we decided to study these qualitatively. We collected a list of the words that appear in the RT and OpeNER datasets which were not found in the embeddings.\n",
      "A revision of each part of speech category revealed that the nouns not found in the embeddings models were often misspellings of more common nouns (fantasi, alientation) or creative use of language (vidgame, dateflick, splatterfests, artsploitation, witlessness, drippiness, naturedness, diciness, gooeyness, baaaaaaaad), both of which are important for categorization.\n",
      "Even more important are the adjectives (unrecommendable, unsuspenseful, uncinematic, unslick, unfakeable, snazzy) and adverbs (heartwarmingly, dullingly, repellently, forgettably, uncharismatically, appallingly) which are not found in these models. These are words that often carry the most sentiment information in a sentence. The number of words lost due to filtering for subjectivity also seems to correlate with the performance of each model, as the subj-Europarl model lost the least number of wordforms and performed the best among the filtered corpora.\n",
      "In order to confirm our theory that this technique can improve representations for under-resourced languages, we perform the same concatenation experiment as in Section 4.2 but with Catalan data. The Catalan Wikipedia used in this experiment was from a wikipedia dump in January, 2016 and contains 182 million tokens. In order to gather task-specific data, we scraped 21,778 reviews in Catalan from www.booking.com which contain a total 4,161,719 tokens. We preprocessed both datasets in a similar fashion (lowercased, sentence tokenized, tokenized) using Freeling (Padró and Stanilovsky, 2012).\n",
      "We test this on the Catalan Aspect-level Sentiment Dataset. This dataset was compiled from 879 Booking.com reviews collected in December of 2016. Three annotators annotated opinion holders, opinion targets, and four levels of sentiment (Strong Positive, Positive, Negative and Strong Negative). Statistics are shown in Table 5.\n",
      "The results on the Catalan dataset (see Table 6) suggest that under-resourced languages benefit more from concatenating vectors trained separately than languages such as English. This is\n",
      "8\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "Training Dev Test Num. phrases 922 62 248 Strong Pos 27.3% 25.8% 27.4% Pos 34.2% 33.9% 34.3% Neg 33.3% 33.9% 33% Strong Neg 5.2% 6.5% 5.2%\n",
      "Table 5: Statistics of the Catalan Aspect-level Sentiment Dataset.\n",
      "50 100 200 300 Wikipedia 44.9 61 72.2 75.4 Booking 55.61 62 72.7 69.5 Appended 61.4 60.4 72.2 70.6 Spliced 49.2 62 70.58 75.4 Booking + Wiki 59.4 62.6 73.3 75.4\n",
      "Table 6: Macro average of classification scores on the Catalan Aspect-level Sentiment Dataset (4 class). Bold indicates the highest performance and underline indicates those that beat the generic baseline.\n",
      "most likely due to the size and breadth of the English Wikipedia corpus. Therefore, for languages that do not have large, broad Wikipedia, we recommend training separate vectors and concatenating them afterwards.\n",
      "In this paper, we have explored several methods for improving sentiment analysis using word embeddings. First, we demonstrated that the subjectivity of a corpus is a viable method to determine the usefulness of a corpus for the task of sentiment analysis. We then show that concatenation of vectors trained on generic and taskspecific datasets outperforms a single representation trained on both datasets. We also showed that it is possible to approximate task-specific data by extracting subjective portions from generic corpora. Finally, we showed that it is possible to use this technique to improve sentiment classification for under-resourced languages, even if there is little task-specific data available.\n",
      "Instead of relying on subjectivity as metric, one could naturally use polarity to determine the usefulness of a passage for training embeddings for a sentiment related task. By removing neutral passages and keeping only those with positive or negative sentiment, it may be possible to achieve similar results to ours. We plan to explore this possi-\n",
      "bility in the future. We also believe that these techniques could improve results on tasks other than sentiment analysis, but we leave this for future work.\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(ACL_path, subset['train'], data['acl']['train'][0])) as json_file:  \n",
    "    file = json.load(json_file)\n",
    "    for key in file['metadata']['sections']:\n",
    "        if key['heading'] is not None:\n",
    "            print(key['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-3b75acc49525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'acl'"
     ]
    }
   ],
   "source": [
    "data['acl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '266.pdf',\n",
       " 'metadata': {'source': 'CRF',\n",
       "  'title': 'Improving sentiment classification with task-specific data',\n",
       "  'authors': [],\n",
       "  'emails': [],\n",
       "  'sections': [{'heading': None,\n",
       "    'text': '1 000\\n011\\n012\\n013\\n014\\n015\\n016\\n017\\n018\\n019\\n020\\n021\\n022\\n023\\n024\\n025\\n026\\n027\\n028\\n029\\n030\\n031\\n032\\n033\\n034\\n035\\n036\\n037\\n038\\n039\\n040\\n041\\n042\\n043\\n044\\n045\\n046\\n047\\n048\\n049\\n061\\n062\\n063\\n064\\n065\\n066\\n067\\n068\\n069\\n070\\n071\\n072\\n073\\n074\\n075\\n076\\n077\\n078\\n079\\n080\\n081\\n082\\n083\\n084\\n085\\n086\\n087\\n088\\n089\\n090\\n091\\n092\\n093\\n094\\n095\\n096\\n097\\n098\\n099'},\n",
       "   {'heading': '1 Introduction',\n",
       "    'text': 'Sentiment analysis techniques requiring only word embeddings as features have proven to be successful (Socher et al., 2013; Tang et al., 2014; Iyyer et al., 2015). Research has also shown the importance of properly initializing the word embeddings, either through careful manipulation of the initialization of random vectors, or more commonly by pretraining the word embeddings (Kim, 2014). The content of the corpora used to train these word embeddings, however, has not been examined in depth.\\nMost research tends toward using larger and larger datasets in order to build high-quality word embeddings, with some studies using corpora on the magnitude of several billion tokens (Mikolov et al., 2013; Pennington et al., 2014). This amount of data, however, is not available in all languages, nor is it necessarily an optimal use of available re-\\nsources. In fact, there is evidence suggesting that tailoring a dataset for a task can achieve better results with less data (Suster et al., 2016; Barnes et al., 2016).\\nThe subjectivity or polarity content of a corpus could provide a good clue to the quality of word embeddings created for the task of sentiment analysis. Pang and Lee (2004) showed that using a pipeline technique where they first classified subjectivity before classifying polarity, they were able to achieve better results than performing classification without filtering subjective sentences. Therefore, we attempt to use subjectivity as a metric for the appropriateness of a corpus for our task.\\nIn this work we aim to:\\n• discover if small task-specific datasets can provide better representations of words than large generic datasets for the task of sentiment analysis\\n• quantify the amount of subjectivity in the data in order to predict the appropriateness of a dataset for the task of sentiment analysis.\\n• discover the best way to combine this information for our task, given a large generic dataset and a smaller domain-specific dataset.\\n• extract the subjective information as a way of approximating a smaller task-specific dataset, given only large generic datasets,\\nAs far as we know, this is the first work on the effects of the subjectivity and task-specificity of a corpus on the creation of word embeddings for the task of sentiment analysis. Given the importance of initialization and the wide-spread use of sentiment analysis techniques using word embeddings, the results are of importance to the community.\\n2\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\n129\\n130\\n131\\n132\\n133\\n134\\n135\\n136\\n137\\n138\\n139\\n140\\n141\\n142\\n143\\n144\\n145\\n146\\n147\\n148\\n149\\n150\\n151\\n152\\n153\\n154\\n155\\n156\\n157\\n158\\n159\\n160\\n161\\n162\\n163\\n164\\n165\\n166\\n167\\n168\\n169\\n170\\n171\\n172\\n173\\n174\\n175\\n176\\n177\\n178\\n179\\n180\\n181\\n182\\n183\\n184\\n185\\n186\\n187\\n188\\n189\\n190\\n191\\n192\\n193\\n194\\n195\\n196\\n197\\n198\\n199\\nThis paper is organized as follows. Section 2 discusses previous work on subjectivity, generic word embeddings and task-specific word embeddings. Section 3 outlines how to quantify the subjectivity of a corpus. Section 4 determines how subjectivity and task-specificity effects word embeddings, and how to combine information from different datasets in order to improve classification. In Section 5 we propose a method to extract task-specific information from generic corpora and, by using the techniques from Section 4.2, improve over our baseline. Finally, in Section 6 we show that this technique of concatenation representations works even better for underresourced languages.'},\n",
       "   {'heading': '2 Related Work', 'text': ''},\n",
       "   {'heading': '2.1 Subjectivity as a filter for polarity',\n",
       "    'text': 'Subjectivity can be a clear indicator of whether a sentence or phrase contains an opinion. In the case of movie or product reviews, this is especially relevant as there are often sentences that seemingly contain sentiment information, yet do not give an opinion towards any relevant aspect: for example, ”The main character tries to protect her good name.” Here, ’good’ gives no relevant sentiment information and could easily be found within a negative review.\\nPang and Lee (2004) use a subjectivity classifier to extract relevant sentences before passing them on to a sentiment classifier. Reducing the amount of irrelevant or potentially misleading data presumably leads to an improvement in polarity classification.\\nThis approach could potentially improve vector representations of words by providing only relevant examples to the word embedding algorithm.'},\n",
       "   {'heading': '2.2 Representing Words as Vectors',\n",
       "    'text': 'Representing words as feature vectors is an idea that has created interest for many years (Deerwester et al., 1990; Lund and Burgess, 1996). The simplest example is to represent a word as a onehot vector the length of the vocabulary, where all entries are zeros except for the index of the word, which is a 1. However, this simple technique does not allow for generalization, as the words ’dog’ and ’cat’ are as different as ’dog’ and ’dogs’.\\nLower-dimensional dense vectors, often known as word embeddings, are able to avoid this problem to a degree. They do this by attempting to\\nforce similar words to occupy a similar area in vector space, given an objective function. The most successful of these applications to date is the Skip Gram with Negative Sampling (SGNS) algorithm (Mikolov et al., 2013). This algorithm applies PMI factorization over an implicit word/feature matrix (Levy and Goldberg, 2014).\\nAlthough generic word embeddings are often used as features for sentiment classifiers (Socher et al., 2013; Kim, 2014), there are good reasons to believe that the generality of these vectors may lead to inferior results. Most state-of-the-art sentiment classifiers are based on deep neural networks. With these architectures it is rather difficult, if not impossible, to find the global optimum and therefore, we must usually be content to find a local optimum. This fact also means that they are sensitive to the initialization of their word embeddings. For this reason, sentiment classifiers initialized with random vectors consistently underperform those initialized with pretrained vectors (Kim, 2014; Iyyer et al., 2015).'},\n",
       "   {'heading': '2.3 Task-specific Word Embeddings',\n",
       "    'text': 'Task-specific word embeddings are a further development of word embeddings. The purpose is to provide a representation that helps a classifier, often by removing or adding task-specific information.\\nThese vectors can be trained as normal and then retrofitted to the task (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015). The advantage of this technique is that any kind of word embedding can be used, as the vectors are modified post training. Faruqui et al. (2015) propose a method to incorporate information from semantic lexicons (WordNet, FrameNet, and ParaphraseNet) into word embeddings.\\nTask-specific word embeddings can also be trained in a supervised manner, using the labels from annotated corpora (Maas et al., 2011; Tang et al., 2014, 2016) or task-specific dictionaries (Gouws and Søgaard, 2015). For the inclusion of sentiment information in word vectors, this is the most common approach.\\nMaas et al. (2011) build a probabilistic model, similar to LDA (Blei et al., 2003), and attempt to model both word relationships and sentiment at the same time. However, their sentiment vectors performed worse than generic word vectors without the sentiment information.\\n3\\n201\\n202\\n203\\n204\\n205\\n206\\n207\\n208\\n209\\n210\\n211\\n212\\n213\\n214\\n215\\n216\\n217\\n218\\n219\\n220\\n221\\n222\\n223\\n224\\n225\\n226\\n227\\n228\\n229\\n230\\n231\\n232\\n233\\n234\\n235\\n236\\n237\\n238\\n239\\n240\\n241\\n242\\n243\\n244\\n245\\n246\\n247\\n248\\n249\\n250\\n251\\n252\\n253\\n254\\n255\\n256\\n257\\n258\\n259\\n260\\n261\\n262\\n263\\n264\\n265\\n266\\n267\\n268\\n269\\n270\\n271\\n272\\n273\\n274\\n275\\n276\\n277\\n278\\n279\\n280\\n281\\n282\\n283\\n284\\n285\\n286\\n287\\n288\\n289\\n290\\n291\\n292\\n293\\n294\\n295\\n296\\n297\\n298\\n299\\nTang et al. (2014) attempt to create sentimentspecific word vectors by modifying the feedforward network architecture used by Collobert and Weston (2011) with a hinge-objective that is designed to maximize both the word representation and the model’s ability to classify polarity. They collected 10 million distantly supervised tweets by searching for positive and negative emoticons. They trained a sentiment classifier and achieved state-of-the-art results on the SemEval2013 shared task (Nakov et al., 2013).\\nBoth of these techniques attempt to use large amounts of annotated data to extract sentiment information, but do not consider different sources of data or ways to combine them. However, it would be beneficial to examine more in depth the type of data that we use to create word vectors.\\nThe work that most closely resembles our is that of Joshi et al. (2015). They explore the effects of using in-domain and generic datasets to create embeddings for e-commerce NER. Their work shows that training on smaller amounts of task-specific data is better than training on large amounts of generic data. Unlike our work, they do not look into ways of combining information from different sources.'},\n",
       "   {'heading': '3 Quantifying the amount of subjectivity in corpora',\n",
       "    'text': ''},\n",
       "   {'heading': '3.1 Datasets',\n",
       "    'text': 'Our first goal was to quantify the amount of subjective information contained in a corpus. We collected the following corpora, which we believe represent both generic datasets often used in the literature (Wikipedia, Europarl, Multi UN) as well as task-specific corpora (Amazon Movie Dataset). All corpora were sentence tokenized, word tokenized and lower-cased using Stanford Core NLP (Manning et al., 2014). We give the details of each corpus in the following subsections as well as in Table 1.\\nWikipedia Corpus\\nAfter downloading a 2016 Wikipedia dump, we removed duplicates and markup using a freely available script1.\\n1http://attardi.github.io/wikiextractor/\\nEnglish Europarl The English part of the Europarl v7 corpus2 (Koehn, 2005) is composed of around 2 million sentences from the European Parliament.\\nMulti UN MultiUN3 is a corpus extracted from the official documents of the United Nations (Eisele and Chen, 2010). We use the English side of the English-Spanish corpus. It contains around 11 million sentences.\\nAmazon Movie Corpus This dataset is composed of more than 7.5 million sentences taken from nearly 1.7 million reviews about movies (McAuley et al., 2015).'},\n",
       "   {'heading': '3.2 Method',\n",
       "    'text': 'In order to determine the amount of subjectivity contained in each corpus, we ran OpinionFinder on each of them. This is a system that was designed for subjectivity and polarity detection (Wilson et al., 2005). It has a high-precision and highrecall subjectivity classifier. The high-precision classifier uses a rule-based method, looking for well-established lexical clues. The high-recall classifier is a naive bayes classifier trained on the MPQA corpus. After running OpinionFinder on each corpus, we retrieved the results for highprecision subjectivity and high-recall subjectivity. In order to quantify the amount of subjectivity, we took the number of sentences reported as subjective and divide them by the total number of sentences in the corpus, (see Formula 1).\\nSubjectivity Score = |subjective sentences| |total sentences|\\n(1)\\nThis gives us an idea of the subjectivity content of one corpus compared to the others, although it is not a precise measurement. We did the same with high-precision and high-recall to get two scores for each corpus (see Table 1). As we expected, the Wikipedia corpus has the lowest scores for subjectivity. This is likely because of its encyclopedic format. The corpora with the highest subjectivity scores are the Amazon (high precision) and Europarl (high recall) corpora.\\n2http://www.statmt.org/europarl 3http://opus.lingfil.uu.se/\\n4\\n301\\n302\\n303\\n304\\n305\\n306\\n307\\n308\\n309\\n310\\n311\\n312\\n313\\n314\\n315\\n316\\n317\\n318\\n319\\n320\\n321\\n322\\n323\\n324\\n325\\n326\\n327\\n328\\n329\\n330\\n331\\n332\\n333\\n334\\n335\\n336\\n337\\n338\\n339\\n340\\n341\\n342\\n343\\n344\\n345\\n346\\n347\\n348\\n349\\n350\\n351\\n352\\n353\\n354\\n355\\n356\\n357\\n358\\n359\\n360\\n361\\n362\\n363\\n364\\n365\\n366\\n367\\n368\\n369\\n370\\n371\\n372\\n373\\n374\\n375\\n376\\n377\\n378\\n379\\n380\\n381\\n382\\n383\\n384\\n385\\n386\\n387\\n388\\n389\\n390\\n391\\n392\\n393\\n394\\n395\\n396\\n397\\n398\\n399\\nWiki Multi UN Europarl Amazon High recall subj. 14.8% 33% 49% 41.5% High precision subj. 2.8% 5.1% 6% 9.3% # Sentences 118,859,889 11,350,967 1,965,734 7,540,838 # Tokens 2,099,860,107 322,056,452 54,474,544 162,390,631 Avg. Sent Length 17.7 28.4 27.7 21.5\\nTable 1: Statistics of corpora used to train word embeddings.'},\n",
       "   {'heading': '4 Experiments', 'text': ''},\n",
       "   {'heading': '4.1 Baseline experiment',\n",
       "    'text': 'We created 50- 100- and 300-dimensional word vectors for each corpus using the Skip-gram with Negative Sampling algorithm (Mikolov et al., 2013) with a window of 10 words, and 5 negative samples.\\nWe tested the word vectors on the Rotten Tomatoes dataset (Pang and Lee, 2005), which contains 10662 annotated sentences. We train on 7996, leave 1066 as a development set and test on 1600.\\nIn order to see how well these representations work on a different domain, we also tested them on the English OpeNER sentiment corpus (Agerri et al., 2013). Specifically, we take the subcorpus from the hotel domain. It contains 3709 opinion phrases annotated for opinion holder, opinion target and four levels of sentiment (Strong Positive, Positive, Negative and Strong Negative). Each training example is a tuple of the opinion holder, opinion target and sentiment phrase, such as (”we”, ”hotel”, ”didn’t like at all”). We train on 2780, leave 186 as a development set and test on 743. This dataset has a class imbalance, which means that macro f1 is a more appropriate metric.\\nFollowing the work of Iyyer et al. (2015), we use a Deep Averaging Network to perform sentiment classification. This neural bag-of-words model does not take the order of the words into consideration, but has proven to give results similar to more sophisticated approaches. Our datasets do not allow us to train on labeled phrases. Also the small size of the training data, especially in the OpeNER dataset, means that updating the word vectors while training the classifier actually hurts classification when using our task-specific vectors. Therefore, we do not update while training.\\nFor each sentence or phrase in the datasets, we create a training example. A training example is the average of the word embeddings for each word in the sentence or phrase. If a word is not found in\\nour word embedding model, it is replaced with a generic ’unknown’ vector, which is randomly initialized. The results are shown in Figures 1 and 2.\\nFigure 1: Accuracy of sentiment classifiers on Rotten Tomatoes dataset\\nAs we can see, the embeddings trained on the much smaller Amazon corpus give consistently better results than the Wikipedia corpus on the RT dataset. However, the vectors trained on Wikipedia perform quite well and even outperform their Amazon trained counterparts on the OpeNER dataset. This is likely due to the fact that the domain for the OpeNER dataset is different (Hotel reviews) compared to the RT dataset (Movie Reviews).'},\n",
       "   {'heading': '4.2 Combining information from different sources',\n",
       "    'text': 'Given that vectors trained on task-specific data regularly outperform those trained on much larger generic datasets, we should ask ourselves if it is possible to combine both of these datasets to get even better results. We compare three methods:\\n1. Appending the task-specific dataset to the generic dataset and training one representation\\n5\\n401\\n402\\n403\\n404\\n405\\n406\\n407\\n408\\n409\\n410\\n411\\n412\\n413\\n414\\n415\\n416\\n417\\n418\\n419\\n420\\n421\\n422\\n423\\n424\\n425\\n426\\n427\\n428\\n429\\n430\\n431\\n432\\n433\\n434\\n435\\n436\\n437\\n438\\n439\\n440\\n441\\n442\\n443\\n444\\n445\\n446\\n447\\n448\\n449\\n450\\n451\\n452\\n453\\n454\\n455\\n456\\n457\\n458\\n459\\n460\\n461\\n462\\n463\\n464\\n465\\n466\\n467\\n468\\n469\\n485\\n486\\n487\\n488\\n489\\n490\\n491\\n492\\n493\\n494\\n495\\n496\\n497\\n498\\n499\\nFigure 2: Accuracy of sentiment classifiers on OpeNER dataset\\n2. Splicing the task-specific dataset into the generic dataset and training one representation\\n3. Training two separate representations and combining them through vector concatenation.\\nAppending involves simply adding the taskspecific Amazon Movie corpus to the Wikpedia corpus and training a single set of vectors on this data. This should improve the representations of those words that are important for the task, as they will receive more positive examples while training the vectors.\\nSplicing is similar to appending, but instead of adding the Amazon sentences, we splice them into the Wikipedia corpus. We alternate sentences from the two corpora until we have exhausted all of the Amazon sentences, after which we add the remaining Wikipedia sentences. As above, we train a single set of vectors.\\nFor concatenation, we train one set of vectors on Wikipedia and a second on the Amazon Movie Corpus separately. Given two m dimensional vectors, we concatenate them, resulting in an m +m dimensional vector, in order to provide both views to the sentiment classifier. For a fair comparison, we concatenate vectors that are half as large as those of the other techniques. This technique may provide better representations for words that often appear in one corpus but not in the other.\\nNote that the embeddings in these three techniques were trained on exactly the same data, the only difference being the method of combining\\nthis data. Our baseline vectors are trained only on the Wikipedia corpus. We also test the 50 dimensional sentiment embeddings (SSWEu) provided by Tang et. al (2014) (see Section 2.3). The results are shown in Table 2.\\nWe can see that including task-specific data with any of the techniques tends to improve over the baseline. This in itself is not surprising. However, it is important to notice that the concatenation of two vectors trained on the two datasets separately always improve the macro f1 score.'},\n",
       "   {'heading': '4.3 How much task-specific data is needed in order to see benefits',\n",
       "    'text': 'Given that we can combine generic and taskspecific data to improve our feature space, we would like to know how much task-specific data is necessary to see these improvements. Therefore, we perform experiments on the RT dataset with increasing amounts of task-specific data. The results are shown in Figure 3.\\nAs we can see from the results, we see improvements over the baseline after 10-12 million tokens. This could potentially benefit low-resource languages which often have access only to Wikipedia pages and little task-specific data.'},\n",
       "   {'heading': '5 Extracting task-specific information from generic datasets',\n",
       "    'text': 'Finally, we ask if there is a way to approximate this task-specific data given only large, generic corpora. This again is a situation that is very common among under-resourced languages. There-\\n6\\n501\\n502\\n503\\n504\\n505\\n506\\n507\\n508\\n509\\n510\\n511\\n512\\n513\\n514\\n515\\n516\\n517\\n518\\n519\\n520\\n521\\n522\\n523\\n524\\n525\\n526\\n527\\n528\\n529\\n530\\n531\\n532\\n533\\n534\\n535\\n536\\n537\\n538\\n539\\n540\\n541\\n542\\n543\\n544\\n545\\n546\\n547\\n548\\n549\\n550\\n551\\n552\\n553\\n554\\n555\\n556\\n557\\n558\\n559\\n560\\n561\\n562\\n563\\n564\\n565\\n566\\n567\\n568\\n569\\n570\\n571\\n572\\n573\\n574\\n575\\n576\\n577\\n578\\n579\\n580\\n581\\n582\\n583\\n584\\n585\\n586\\n587\\n588\\n589\\n590\\n591\\n592\\n593\\n594\\n595\\n596\\n597\\n598\\n599\\nRT OpeNER 50 100 200 300 50 100 200 300\\nWikipedia Baseline acc 66.2 71.9 71.4 71.3 77.6 78.2 82.1 83.6 f1 59.8 72.4 .67.4 .67.5 37.2 36.7 39.8 40.6 SSWEu acc 66.3 – – – 81 – – – f1 56.5 – – – 39.4 – – – Amazon acc 73.8 76.3 76.7 76.9 73.5 73.8 77.8 76.9 f1 69.9 74.7 74.6 75.3 34.9 33.6 37.4 38 Appended acc 69.4 73.8 74.5 74.3 77.6 76.5 82.7 78.4 f1 65.1 75 72.5 72.8 36.9 35.8 40.5 36.4 Spliced acc 70.8 71.2 73.1 73.6 77.4 73.9 84.6 82.7 f1 62.5 73.4 71 70 37.2 32.9 41.4 39.9 Amazon + Wiki acc 74.4 75.2 75.9 75.9 64.2 65.7 69.4 72.3 f1 72.5 75 78 77.7 57.8 62.4 63.8 68\\nTable 2: Accuracy and F1 of sentiment classifier with embeddings trained on Wikipedia and Amazon corpora with different techniques. For the OpeNER dataset, accuracy is the macro average. Bold indicates the highest performance and underline indicates those that beat the generic baseline.\\nfore, we extract the sentences labeled as subjective from the Wikipedia, Europarl and Multi UN corpora, referred to from here on as subj-Wikipedia, subj-Europarl, subj-MultiUN. We train word embeddings on each of these and test them as in Section 4.2. We also report macro-averaged f1 scores. Results are shown in Table 3.\\nWe can see that by filtering the corpora for subjectivity, we lose a degree of accuracy, but gain in f1. The poor f1 scores of the Wikipedia baseline on the OpeNER dataset are due to the fact that the classifier nearly always chooses the two majority classes and completely ignores the others. Including more subjective information seems to ameliorate this situation. The improvements in f1 also coincide with the subjectivity level predicted by our method. This suggests that it is possible to predict the amount of improvement that you can gain by extracting subjective data from a generic dataset.'},\n",
       "   {'heading': '5.1 Lexical Overlap',\n",
       "    'text': 'Given that the classification error is still relatively large, our hypothesis was that many words which are important for classification are not found in the corpora used to train the word embeddings. In order to test this possibility, we found the lexical overlap in vocabulary between the corpora used to train the word embeddings and the RT and OpeNER corpora. Let Vtr be the set of words in a training corpus and Vtest be the set of words in each test corpus test.\\nLexicalOverlap = |Vtest| ∩ |Vtr| |Vtest|\\n(2)\\nIf the lexical overlap of the corpora used to train our representations (Wikipedia, Europarl, Multiun) and the datasets we use to test our classifier (RT, OpeNER) is small, this will negatively affect the results of the classifier, as it will lack necessary information during both training and testing. We also test for lexical diversity of the filtered subjectivity training corpora as a measure of vocabulary. We use the Average Type Token Ratio (ATTR) as a measure. Let k be a corpus, nk be a subset of n tokens of k, Vn = the vocabulary of n, and Tn = the tokens of n.\\nAverage Type Token Ratio = 1\\nn ∗ N∑ n=1 |Vn| |Tn| (3)\\nWe set n to 1000. The results are shown in Table 4.\\nAs we can see in Table 4, the number of words that do not have a vector representation in the full Wikipedia model is much smaller than all other models. Therefore, the Wikipedia word embeddings have a larger vocabulary yet they do not always have a representation that is helpful for the task. The more subjective datasets may lack some vocabulary, but it seems that the representations that they have are able to overcome the sparsity.\\n7\\n601\\n602\\n603\\n604\\n605\\n606\\n607\\n608\\n609\\n610\\n611\\n612\\n613\\n614\\n615\\n616\\n617\\n618\\n619\\n620\\n621\\n622\\n623\\n624\\n625\\n626\\n627\\n628\\n629\\n630\\n631\\n632\\n633\\n634\\n635\\n636\\n637\\n638\\n639\\n640\\n641\\n642\\n643\\n644\\n645\\n646\\n647\\n648\\n649\\n650\\n651\\n652\\n653\\n654\\n655\\n656\\n657\\n658\\n659\\n660\\n661\\n662\\n663\\n664\\n665\\n666\\n667\\n668\\n669\\n670\\n671\\n672\\n673\\n674\\n675\\n676\\n677\\n678\\n679\\n680\\n681\\n682\\n683\\n684\\n685\\n686\\n687\\n688\\n689\\n690\\n691\\n692\\n693\\n694\\n695\\n696\\n697\\n698\\n699\\nRT OpeNER 50 100 200 300 50 100 200 300\\nWikipedia Baseline acc 66.2 71.9 71.4 71.3 77.6 78.2 82.1 83.6 f1 59.8 72.4 67.4 67.5 37.2 36.7 39.8 40.6 subj-Wiki + Wiki acc 66.9 69.1 70.3 72.4 62.1 67.6 67.8 70.3 f1 67.5 71.8 74.6 75.5 51.2 58.3 57.9 60.8 subj-Multiun + Wiki acc 68.4 69.6 69.3 73.4 65.1 68.3 68.2 71 f1 69 73.2 74 75.6 60.2 60.7 57.7 62.8 subj-Europarl + Wiki acc 68.2 68.4 71.3 74.1 63.7 68.3 69.6 71.7 f1 69.7 73.7 75.5 76.2 56.9 59.5 66.3 63.4\\nTable 3: Accuracy and F1 scores of sentiment classifier trained on concatenation of filtered subjectivity vectors and wikipedia vectors. Bold indicates the highest performance and underline indicates those that beat the Wikipedia baseline.\\nWiki Amazon subj-Wiki subj-MulUN subj-Euro # tokens 1.8B˜ 162M˜ 24M˜ 43M˜ 33M˜ ATTR 41.5 39.9 44.3 36.7 39.4 Overlap RT-Train 83.7 61.6 73.3 70.9 68.6 Overlap RT-Test 87.5 69.9 76.7 75.3 74 Overlap OpeNER-Train 89.1 70.3 81.3 78.1 75 Overlap OpeNER-Test 96 89.1 92 90 90\\nTable 4: Average Type/Token ratio (ATTR) and overlap with test vocabulary in each training corpus.'},\n",
       "   {'heading': '5.2 Types of missing words',\n",
       "    'text': 'After seeing the number of words which were missing, we decided to study these qualitatively. We collected a list of the words that appear in the RT and OpeNER datasets which were not found in the embeddings.\\nA revision of each part of speech category revealed that the nouns not found in the embeddings models were often misspellings of more common nouns (fantasi, alientation) or creative use of language (vidgame, dateflick, splatterfests, artsploitation, witlessness, drippiness, naturedness, diciness, gooeyness, baaaaaaaad), both of which are important for categorization.\\nEven more important are the adjectives (unrecommendable, unsuspenseful, uncinematic, unslick, unfakeable, snazzy) and adverbs (heartwarmingly, dullingly, repellently, forgettably, uncharismatically, appallingly) which are not found in these models. These are words that often carry the most sentiment information in a sentence. The number of words lost due to filtering for subjectivity also seems to correlate with the performance of each model, as the subj-Europarl model lost the least number of wordforms and performed the best among the filtered corpora.'},\n",
       "   {'heading': '6 Experiments on under-resourced languages',\n",
       "    'text': 'In order to confirm our theory that this technique can improve representations for under-resourced languages, we perform the same concatenation experiment as in Section 4.2 but with Catalan data. The Catalan Wikipedia used in this experiment was from a wikipedia dump in January, 2016 and contains 182 million tokens. In order to gather task-specific data, we scraped 21,778 reviews in Catalan from www.booking.com which contain a total 4,161,719 tokens. We preprocessed both datasets in a similar fashion (lowercased, sentence tokenized, tokenized) using Freeling (Padró and Stanilovsky, 2012).\\nWe test this on the Catalan Aspect-level Sentiment Dataset. This dataset was compiled from 879 Booking.com reviews collected in December of 2016. Three annotators annotated opinion holders, opinion targets, and four levels of sentiment (Strong Positive, Positive, Negative and Strong Negative). Statistics are shown in Table 5.\\nThe results on the Catalan dataset (see Table 6) suggest that under-resourced languages benefit more from concatenating vectors trained separately than languages such as English. This is\\n8\\n701\\n702\\n703\\n704\\n705\\n706\\n707\\n708\\n709\\n710\\n711\\n712\\n713\\n714\\n715\\n716\\n717\\n718\\n719\\n720\\n721\\n722\\n723\\n724\\n725\\n726\\n727\\n728\\n729\\n730\\n731\\n732\\n733\\n734\\n735\\n736\\n737\\n738\\n739\\n740\\n741\\n742\\n743\\n744\\n745\\n746\\n747\\n748\\n749\\n750\\n751\\n752\\n753\\n754\\n755\\n756\\n757\\n758\\n759\\n760\\n761\\n762\\n763\\n764\\n765\\n766\\n767\\n768\\n769\\n770\\n771\\n772\\n773\\n774\\n775\\n776\\n777\\n778\\n779\\n780\\n781\\n782\\n783\\n784\\n785\\n786\\n787\\n788\\n789\\n790\\n791\\n792\\n793\\n794\\n795\\n796\\n797\\n798\\n799\\nTraining Dev Test Num. phrases 922 62 248 Strong Pos 27.3% 25.8% 27.4% Pos 34.2% 33.9% 34.3% Neg 33.3% 33.9% 33% Strong Neg 5.2% 6.5% 5.2%\\nTable 5: Statistics of the Catalan Aspect-level Sentiment Dataset.\\n50 100 200 300 Wikipedia 44.9 61 72.2 75.4 Booking 55.61 62 72.7 69.5 Appended 61.4 60.4 72.2 70.6 Spliced 49.2 62 70.58 75.4 Booking + Wiki 59.4 62.6 73.3 75.4\\nTable 6: Macro average of classification scores on the Catalan Aspect-level Sentiment Dataset (4 class). Bold indicates the highest performance and underline indicates those that beat the generic baseline.\\nmost likely due to the size and breadth of the English Wikipedia corpus. Therefore, for languages that do not have large, broad Wikipedia, we recommend training separate vectors and concatenating them afterwards.'},\n",
       "   {'heading': '7 Conclusion and Future Work',\n",
       "    'text': 'In this paper, we have explored several methods for improving sentiment analysis using word embeddings. First, we demonstrated that the subjectivity of a corpus is a viable method to determine the usefulness of a corpus for the task of sentiment analysis. We then show that concatenation of vectors trained on generic and taskspecific datasets outperforms a single representation trained on both datasets. We also showed that it is possible to approximate task-specific data by extracting subjective portions from generic corpora. Finally, we showed that it is possible to use this technique to improve sentiment classification for under-resourced languages, even if there is little task-specific data available.\\nInstead of relying on subjectivity as metric, one could naturally use polarity to determine the usefulness of a passage for training embeddings for a sentiment related task. By removing neutral passages and keeping only those with positive or negative sentiment, it may be possible to achieve similar results to ours. We plan to explore this possi-\\nbility in the future. We also believe that these techniques could improve results on tasks other than sentiment analysis, but we leave this for future work.'}],\n",
       "  'references': [{'title': 'OpeNER: Open polarity enhanced named entity recognition',\n",
       "    'author': ['Rodrigo Agerri',\n",
       "     'Montse Cuadros',\n",
       "     'Sean Gaines',\n",
       "     'German Rigau.'],\n",
       "    'venue': 'Sociedad Española para el Procesamiento del Lenguaje Natural 51(Septiembre):215–218.',\n",
       "    'citeRegEx': 'Agerri et al\\\\.,? 2013',\n",
       "    'shortCiteRegEx': 'Agerri et al\\\\.',\n",
       "    'year': 2013},\n",
       "   {'title': 'Exploring distributional representations and machine translation for aspect-based cross-lingual sentiment classification',\n",
       "    'author': ['Jeremy Barnes', 'Patrik Lambert', 'Toni Badia.'],\n",
       "    'venue': 'Proceedings of COLING 2016, the 26th Interna-',\n",
       "    'citeRegEx': 'Barnes et al\\\\.,? 2016',\n",
       "    'shortCiteRegEx': 'Barnes et al\\\\.',\n",
       "    'year': 2016},\n",
       "   {'title': 'Latent dirichlet allocation',\n",
       "    'author': ['David M. Blei', 'Andrew Y. Ng', 'Michael I. Jordan.'],\n",
       "    'venue': 'J. Mach. Learn. Res. 3:993–1022. http://dl.acm.org/citation.cfm?id=944919.944937.',\n",
       "    'citeRegEx': 'Blei et al\\\\.,? 2003',\n",
       "    'shortCiteRegEx': 'Blei et al\\\\.',\n",
       "    'year': 2003},\n",
       "   {'title': 'Natural language processing (almost) from scratch',\n",
       "    'author': ['Ronan Collobert',\n",
       "     'Jason Weston',\n",
       "     'Léon Bottou',\n",
       "     'Michael Karlen',\n",
       "     'Koray Kavukcuoglu',\n",
       "     'Pavel Kuksa.'],\n",
       "    'venue': 'J. Mach. Learn. Res. 12:2493–2537. http://dl.acm.org/citation.cfm?id=1953048.2078186.',\n",
       "    'citeRegEx': 'Collobert et al\\\\.,? 2011',\n",
       "    'shortCiteRegEx': 'Collobert et al\\\\.',\n",
       "    'year': 2011},\n",
       "   {'title': 'Indexing by latent semantic analysis',\n",
       "    'author': ['Scott Deerwester',\n",
       "     'Susan T. Dumais',\n",
       "     'George W. Furnas',\n",
       "     'Thomas K. Landauer',\n",
       "     'Richard Harshman.'],\n",
       "    'venue': 'Journal of the American Society for Information Science 41(6):391–407.',\n",
       "    'citeRegEx': 'Deerwester et al\\\\.,? 1990',\n",
       "    'shortCiteRegEx': 'Deerwester et al\\\\.',\n",
       "    'year': 1990},\n",
       "   {'title': 'Multiun: A multilingual corpus from united nation documents',\n",
       "    'author': ['Andreas Eisele', 'Yu Chen.'],\n",
       "    'venue': 'Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel',\n",
       "    'citeRegEx': 'Eisele and Chen.,? 2010',\n",
       "    'shortCiteRegEx': 'Eisele and Chen.',\n",
       "    'year': 2010},\n",
       "   {'title': 'Retrofitting word vectors to semantic lexicons',\n",
       "    'author': ['Manaal Faruqui',\n",
       "     'Jesse Dodge',\n",
       "     'Sujay Kumar Jauhar',\n",
       "     'Chris Dyer',\n",
       "     'Eduard Hovy',\n",
       "     'Noah A. Smith.'],\n",
       "    'venue': 'Proceedings of the 2015 Conference of the North American Chapter of the Associa-',\n",
       "    'citeRegEx': 'Faruqui et al\\\\.,? 2015',\n",
       "    'shortCiteRegEx': 'Faruqui et al\\\\.',\n",
       "    'year': 2015},\n",
       "   {'title': 'Simple task-specific bilingual word embeddings',\n",
       "    'author': ['Stephan Gouws', 'Anders Søgaard.'],\n",
       "    'venue': 'Proceedings of the 2015 Conference of the',\n",
       "    'citeRegEx': 'Gouws and Søgaard.,? 2015',\n",
       "    'shortCiteRegEx': 'Gouws and Søgaard.',\n",
       "    'year': 2015},\n",
       "   {'title': 'Deep unordered composition rivals syntactic methods for text classification',\n",
       "    'author': ['Mohit Iyyer',\n",
       "     'Varun Manjunatha',\n",
       "     'Jordan Boyd-Graber',\n",
       "     'Hal Daumé III.'],\n",
       "    'venue': 'Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-',\n",
       "    'citeRegEx': 'Iyyer et al\\\\.,? 2015',\n",
       "    'shortCiteRegEx': 'Iyyer et al\\\\.',\n",
       "    'year': 2015},\n",
       "   {'title': 'Distributed word representations improve ner for e-commerce',\n",
       "    'author': ['Mahesh Joshi',\n",
       "     'Ethan Hart',\n",
       "     'Mirko Vogel',\n",
       "     'JeanDavid Ruvini.'],\n",
       "    'venue': 'Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing. Associa-',\n",
       "    'citeRegEx': 'Joshi et al\\\\.,? 2015',\n",
       "    'shortCiteRegEx': 'Joshi et al\\\\.',\n",
       "    'year': 2015},\n",
       "   {'title': 'Convolutional neural networks for sentence classification',\n",
       "    'author': ['Yoon Kim.'],\n",
       "    'venue': 'Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, pages 1746–',\n",
       "    'citeRegEx': 'Kim.,? 2014',\n",
       "    'shortCiteRegEx': 'Kim.',\n",
       "    'year': 2014},\n",
       "   {'title': 'Europarl: A parallel corpus for statistical machine translation',\n",
       "    'author': ['Philipp Koehn.'],\n",
       "    'venue': 'MT Summit.',\n",
       "    'citeRegEx': 'Koehn.,? 2005',\n",
       "    'shortCiteRegEx': 'Koehn.',\n",
       "    'year': 2005},\n",
       "   {'title': 'Neural word embedding as implicit matrix factorization',\n",
       "    'author': ['Omer Levy', 'Yoav Goldberg.'],\n",
       "    'venue': 'Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems',\n",
       "    'citeRegEx': 'Levy and Goldberg.,? 2014',\n",
       "    'shortCiteRegEx': 'Levy and Goldberg.',\n",
       "    'year': 2014},\n",
       "   {'title': 'Producing highdimensional semantic spaces from lexical cooccurrence',\n",
       "    'author': ['K. Lund', 'C. Burgess.'],\n",
       "    'venue': 'Behavior Research Methods Instruments and Computers 28(2):203–208.',\n",
       "    'citeRegEx': 'Lund and Burgess.,? 1996',\n",
       "    'shortCiteRegEx': 'Lund and Burgess.',\n",
       "    'year': 1996},\n",
       "   {'title': 'Learning word vectors for sentiment analysis',\n",
       "    'author': ['Andrew L. Maas',\n",
       "     'Raymond E. Daly',\n",
       "     'Peter T. Pham',\n",
       "     'Dan Huang',\n",
       "     'Andrew Y. Ng',\n",
       "     'Christopher Potts.'],\n",
       "    'venue': 'Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human',\n",
       "    'citeRegEx': 'Maas et al\\\\.,? 2011',\n",
       "    'shortCiteRegEx': 'Maas et al\\\\.',\n",
       "    'year': 2011},\n",
       "   {'title': 'The stanford corenlp natural language processing toolkit',\n",
       "    'author': ['Christopher Manning',\n",
       "     'Mihai Surdeanu',\n",
       "     'John Bauer',\n",
       "     'Jenny Finkel',\n",
       "     'Steven Bethard',\n",
       "     'David McClosky.'],\n",
       "    'venue': 'Proceedings of 52nd Annual Meeting of the Asso-',\n",
       "    'citeRegEx': 'Manning et al\\\\.,? 2014',\n",
       "    'shortCiteRegEx': 'Manning et al\\\\.',\n",
       "    'year': 2014},\n",
       "   {'title': 'Efficient estimation of word',\n",
       "    'author': ['frey Dean'],\n",
       "    'venue': None,\n",
       "    'citeRegEx': 'Dean.,? \\\\Q2013\\\\E',\n",
       "    'shortCiteRegEx': 'Dean.',\n",
       "    'year': 2013},\n",
       "   {'title': 'Bilingual learning of multi-sense embeddings with discrete autoencoders',\n",
       "    'author': ['Simon Suster', 'Ivan Titov', 'Gertjan van Noord.'],\n",
       "    'venue': 'CoRR abs(1603.09128. http://www.aclweb.org/anthology/N/N16/N161160.pdf.',\n",
       "    'citeRegEx': 'Suster et al\\\\.,? 2016',\n",
       "    'shortCiteRegEx': 'Suster et al\\\\.',\n",
       "    'year': 2016},\n",
       "   {'title': 'Sentiment embeddings with applications to sentiment analysis',\n",
       "    'author': ['Duyu Tang',\n",
       "     'Furu Wei',\n",
       "     'Bing Qin',\n",
       "     'Nan Yang',\n",
       "     'Ting Liu',\n",
       "     'Ming Zhou.'],\n",
       "    'venue': 'IEEE Trans. on Knowl. and Data Eng. 28(2):496–509. https://doi.org/10.1109/TKDE.2015.2489653.',\n",
       "    'citeRegEx': 'Tang et al\\\\.,? 2016',\n",
       "    'shortCiteRegEx': 'Tang et al\\\\.',\n",
       "    'year': 2016},\n",
       "   {'title': 'Learning sentimentspecific word embedding for twitter sentiment classification',\n",
       "    'author': ['Duyu Tang',\n",
       "     'Furu Wei',\n",
       "     'Nan Yang',\n",
       "     'Ming Zhou',\n",
       "     'Ting Liu',\n",
       "     'Bing Qin.'],\n",
       "    'venue': 'Proceedings of the 52nd Annual Meeting of the Association for Computational Linguis-',\n",
       "    'citeRegEx': 'Tang et al\\\\.,? 2014',\n",
       "    'shortCiteRegEx': 'Tang et al\\\\.',\n",
       "    'year': 2014},\n",
       "   {'title': 'Opinionfinder: A system for subjectivity analysis',\n",
       "    'author': ['Theresa Wilson',\n",
       "     'Paul Hoffman',\n",
       "     'Swapna Somasundaran',\n",
       "     'Jason Kessler',\n",
       "     'Janice Wiebe',\n",
       "     'Yejin Choi',\n",
       "     'Claire Cardie',\n",
       "     'Ellen Riloff',\n",
       "     'Siddharth Patwardhan.'],\n",
       "    'venue': 'Proceedings of the Conference on Hu-',\n",
       "    'citeRegEx': 'Wilson et al\\\\.,? 2005',\n",
       "    'shortCiteRegEx': 'Wilson et al\\\\.',\n",
       "    'year': 2005},\n",
       "   {'title': 'Rc-net: A general framework for incorporating knowledge into word representations',\n",
       "    'author': ['Chang Xu',\n",
       "     'Yalong Bai',\n",
       "     'Jiang Bian',\n",
       "     'Bin Gao',\n",
       "     'Gang Wang',\n",
       "     'Xiaoguang Liu',\n",
       "     'Tie-Yan Liu.'],\n",
       "    'venue': 'Proceedings of the 23rd ACM In-',\n",
       "    'citeRegEx': 'Xu et al\\\\.,? 2014',\n",
       "    'shortCiteRegEx': 'Xu et al\\\\.',\n",
       "    'year': 2014},\n",
       "   {'title': 'Improving lexical embeddings with semantic knowledge',\n",
       "    'author': ['Mo Yu', 'Mark Dredze.'],\n",
       "    'venue': 'Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational',\n",
       "    'citeRegEx': 'Yu and Dredze.,? 2014',\n",
       "    'shortCiteRegEx': 'Yu and Dredze.',\n",
       "    'year': 2014}],\n",
       "  'referenceMentions': [{'referenceID': 19,\n",
       "    'context': 'Sentiment analysis techniques requiring only word embeddings as features have proven to be successful (Socher et al., 2013; Tang et al., 2014; Iyyer et al., 2015).',\n",
       "    'startOffset': 102,\n",
       "    'endOffset': 162},\n",
       "   {'referenceID': 8,\n",
       "    'context': 'Sentiment analysis techniques requiring only word embeddings as features have proven to be successful (Socher et al., 2013; Tang et al., 2014; Iyyer et al., 2015).',\n",
       "    'startOffset': 102,\n",
       "    'endOffset': 162},\n",
       "   {'referenceID': 10,\n",
       "    'context': 'Research has also shown the importance of properly initializing the word embeddings, either through careful manipulation of the initialization of random vectors, or more commonly by pretraining the word embeddings (Kim, 2014).',\n",
       "    'startOffset': 214,\n",
       "    'endOffset': 225},\n",
       "   {'referenceID': 17,\n",
       "    'context': 'In fact, there is evidence suggesting that tailoring a dataset for a task can achieve better results with less data (Suster et al., 2016; Barnes et al., 2016).',\n",
       "    'startOffset': 116,\n",
       "    'endOffset': 158},\n",
       "   {'referenceID': 1,\n",
       "    'context': 'In fact, there is evidence suggesting that tailoring a dataset for a task can achieve better results with less data (Suster et al., 2016; Barnes et al., 2016).',\n",
       "    'startOffset': 116,\n",
       "    'endOffset': 158},\n",
       "   {'referenceID': 1,\n",
       "    'context': ', 2016; Barnes et al., 2016). The subjectivity or polarity content of a corpus could provide a good clue to the quality of word embeddings created for the task of sentiment analysis. Pang and Lee (2004) showed that using a pipeline technique where they first classified subjectivity before classifying polarity, they were able to achieve better results than performing classification without filtering subjective sentences.',\n",
       "    'startOffset': 8,\n",
       "    'endOffset': 203},\n",
       "   {'referenceID': 4,\n",
       "    'context': '2 Representing Words as Vectors Representing words as feature vectors is an idea that has created interest for many years (Deerwester et al., 1990; Lund and Burgess, 1996).',\n",
       "    'startOffset': 122,\n",
       "    'endOffset': 171},\n",
       "   {'referenceID': 13,\n",
       "    'context': '2 Representing Words as Vectors Representing words as feature vectors is an idea that has created interest for many years (Deerwester et al., 1990; Lund and Burgess, 1996).',\n",
       "    'startOffset': 122,\n",
       "    'endOffset': 171},\n",
       "   {'referenceID': 12,\n",
       "    'context': 'This algorithm applies PMI factorization over an implicit word/feature matrix (Levy and Goldberg, 2014).',\n",
       "    'startOffset': 78,\n",
       "    'endOffset': 103},\n",
       "   {'referenceID': 10,\n",
       "    'context': 'Although generic word embeddings are often used as features for sentiment classifiers (Socher et al., 2013; Kim, 2014), there are good reasons to believe that the generality of these vectors may lead to inferior results.',\n",
       "    'startOffset': 86,\n",
       "    'endOffset': 118},\n",
       "   {'referenceID': 10,\n",
       "    'context': 'For this reason, sentiment classifiers initialized with random vectors consistently underperform those initialized with pretrained vectors (Kim, 2014; Iyyer et al., 2015).',\n",
       "    'startOffset': 139,\n",
       "    'endOffset': 170},\n",
       "   {'referenceID': 8,\n",
       "    'context': 'For this reason, sentiment classifiers initialized with random vectors consistently underperform those initialized with pretrained vectors (Kim, 2014; Iyyer et al., 2015).',\n",
       "    'startOffset': 139,\n",
       "    'endOffset': 170},\n",
       "   {'referenceID': 22,\n",
       "    'context': 'These vectors can be trained as normal and then retrofitted to the task (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015).',\n",
       "    'startOffset': 72,\n",
       "    'endOffset': 132},\n",
       "   {'referenceID': 21,\n",
       "    'context': 'These vectors can be trained as normal and then retrofitted to the task (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015).',\n",
       "    'startOffset': 72,\n",
       "    'endOffset': 132},\n",
       "   {'referenceID': 6,\n",
       "    'context': 'These vectors can be trained as normal and then retrofitted to the task (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015).',\n",
       "    'startOffset': 72,\n",
       "    'endOffset': 132},\n",
       "   {'referenceID': 14,\n",
       "    'context': 'Task-specific word embeddings can also be trained in a supervised manner, using the labels from annotated corpora (Maas et al., 2011; Tang et al., 2014, 2016) or task-specific dictionaries (Gouws and Søgaard, 2015).',\n",
       "    'startOffset': 114,\n",
       "    'endOffset': 158},\n",
       "   {'referenceID': 7,\n",
       "    'context': ', 2014, 2016) or task-specific dictionaries (Gouws and Søgaard, 2015).',\n",
       "    'startOffset': 44,\n",
       "    'endOffset': 69},\n",
       "   {'referenceID': 2,\n",
       "    'context': '(2011) build a probabilistic model, similar to LDA (Blei et al., 2003), and attempt to model both word relationships and sentiment at the same time.',\n",
       "    'startOffset': 51,\n",
       "    'endOffset': 70},\n",
       "   {'referenceID': 5,\n",
       "    'context': ', 2014; Faruqui et al., 2015). The advantage of this technique is that any kind of word embedding can be used, as the vectors are modified post training. Faruqui et al. (2015) propose a method to incorporate information from semantic lexicons (WordNet, FrameNet, and ParaphraseNet) into word embeddings.',\n",
       "    'startOffset': 8,\n",
       "    'endOffset': 176},\n",
       "   {'referenceID': 5,\n",
       "    'context': ', 2014; Faruqui et al., 2015). The advantage of this technique is that any kind of word embedding can be used, as the vectors are modified post training. Faruqui et al. (2015) propose a method to incorporate information from semantic lexicons (WordNet, FrameNet, and ParaphraseNet) into word embeddings. Task-specific word embeddings can also be trained in a supervised manner, using the labels from annotated corpora (Maas et al., 2011; Tang et al., 2014, 2016) or task-specific dictionaries (Gouws and Søgaard, 2015). For the inclusion of sentiment information in word vectors, this is the most common approach. Maas et al. (2011) build a probabilistic model, similar to LDA (Blei et al.',\n",
       "    'startOffset': 8,\n",
       "    'endOffset': 633},\n",
       "   {'referenceID': 9,\n",
       "    'context': 'The work that most closely resembles our is that of Joshi et al. (2015). They explore the effects of using in-domain and generic datasets to create embeddings for e-commerce NER.',\n",
       "    'startOffset': 52,\n",
       "    'endOffset': 72},\n",
       "   {'referenceID': 15,\n",
       "    'context': 'All corpora were sentence tokenized, word tokenized and lower-cased using Stanford Core NLP (Manning et al., 2014).',\n",
       "    'startOffset': 92,\n",
       "    'endOffset': 114},\n",
       "   {'referenceID': 11,\n",
       "    'context': 'io/wikiextractor/ English Europarl The English part of the Europarl v7 corpus2 (Koehn, 2005) is composed of around 2 million sentences from the European Parliament.',\n",
       "    'startOffset': 79,\n",
       "    'endOffset': 92},\n",
       "   {'referenceID': 5,\n",
       "    'context': 'Multi UN MultiUN3 is a corpus extracted from the official documents of the United Nations (Eisele and Chen, 2010).',\n",
       "    'startOffset': 90,\n",
       "    'endOffset': 113},\n",
       "   {'referenceID': 20,\n",
       "    'context': 'This is a system that was designed for subjectivity and polarity detection (Wilson et al., 2005).',\n",
       "    'startOffset': 75,\n",
       "    'endOffset': 96},\n",
       "   {'referenceID': 0,\n",
       "    'context': 'In order to see how well these representations work on a different domain, we also tested them on the English OpeNER sentiment corpus (Agerri et al., 2013).',\n",
       "    'startOffset': 134,\n",
       "    'endOffset': 155},\n",
       "   {'referenceID': 0,\n",
       "    'context': 'In order to see how well these representations work on a different domain, we also tested them on the English OpeNER sentiment corpus (Agerri et al., 2013). Specifically, we take the subcorpus from the hotel domain. It contains 3709 opinion phrases annotated for opinion holder, opinion target and four levels of sentiment (Strong Positive, Positive, Negative and Strong Negative). Each training example is a tuple of the opinion holder, opinion target and sentiment phrase, such as (”we”, ”hotel”, ”didn’t like at all”). We train on 2780, leave 186 as a development set and test on 743. This dataset has a class imbalance, which means that macro f1 is a more appropriate metric. Following the work of Iyyer et al. (2015), we use a Deep Averaging Network to perform sentiment classification.',\n",
       "    'startOffset': 135,\n",
       "    'endOffset': 722}],\n",
       "  'year': 2017,\n",
       "  'abstractText': 'Current state-of-the-art sentiment analysis techniques rely heavily on pre-trained word embeddings. However, the data used to train these embeddings normally comes from large, generic datasets, such as Wikipedia or GoogleNews, which may not include enough task-specific information to create reliable representations. This paper proposes a method to determine the subjectivity of a corpus using available tools and shows that word embeddings trained on task-specific corpora tend to outperform those trained on generic data. We then examine ways to combine information from generic and task-specific datasets and finally demonstrate that our method can work well for under-resourced languages.',\n",
       "  'creator': 'LaTeX with hyperref package'}}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
